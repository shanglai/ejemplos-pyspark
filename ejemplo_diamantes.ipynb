{"cells":[{"cell_type":"code","source":["# Manipulación de Datos"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["## Lectura de archivo"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["diamonds = sqlContext.read.format('com.databricks.spark.csv').options(\nheader='true', inferSchema='true').load('/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv')"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["display(diamonds)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["diamonds.printSchema()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["print(diamonds.count())"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display(diamonds.select(\"color\").distinct().collect())"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["## Calculando los valores - precio / carat"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import *\ndiamondsCast = diamonds.withColumn(\"price\", diamonds[\"price\"].cast(DoubleType()))\ncarat_avgPrice = (diamondsCast\n               .groupBy(\"carat\")\n               .avg(\"price\")\n               .withColumnRenamed(\"avg(price)\", \"avgPrice\")\n               .orderBy(desc(\"avgPrice\")))\ncarat_avgPrice.show(10)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["### No sirve de mucho hacer un agrupado por una columna con valores continuos, así que podemos redondear un poco:\ndiamondsCast2= diamondsCast.withColumn(\"carat2\",round(col(\"carat\")))\ndiamondsCast2.show(10)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["(diamondsCast2.groupBy(\"carat2\").avg(\"price\").withColumnRenamed(\"avg(price)\", \"avgPrice\").orderBy(desc(\"avgPrice\"))).show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# También podríamos obtener el máximo de tabla por corte\nfrom pyspark.sql.window import Window\nimport pyspark.sql.functions as func\nwindowSpec = Window.partitionBy(diamondsCast['cut']).orderBy(diamondsCast['table'].desc())\ndiamondsCast.select(\"cut\",\"table\",func.row_number().over(windowSpec).alias('lugar')).filter(\"lugar=1\").show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["diamDf= diamondsCast.rdd\n# Suma del precio por claridad\nclaridadPrecio= diamDf.map(lambda x: (x.clarity,x.price)).reduceByKey(lambda x,y:x+y)\ndisplay(claridadPrecio.collect())"],"metadata":{},"outputs":[],"execution_count":13}],"metadata":{"name":"ejemplo_diamantes","notebookId":228163480878882},"nbformat":4,"nbformat_minor":0}
